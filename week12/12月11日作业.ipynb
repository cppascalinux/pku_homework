{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import time\n",
    "from visdom import Visdom\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "viz = Visdom(server='http://127.0.0.1', port=8097)\n",
    "assert viz.check_connection()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、修改loss曲线\n",
    "\n",
    "12月9日的作业loss曲线一次只能显示一个epoch，并不连续。请你修改代码（修改方式不限），使所有epoch的loss都连续地显示在同一个window里。把修改后的代码补充在下面。在注释中标明自己修改的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting up a new session...\n",
      "Epoch: 0 Train      Avg. Loss: 0.000947, Accuracy: 0.922000\n",
      "Epoch: 0 Validation Avg. Loss: 0.000056, Accuracy: 0.982800\n",
      "Epoch: 1 Train      Avg. Loss: 0.000390, Accuracy: 0.969267\n",
      "Epoch: 1 Validation Avg. Loss: 0.000035, Accuracy: 0.988500\n",
      "Epoch: 2 Train      Avg. Loss: 0.000336, Accuracy: 0.973033\n",
      "Epoch: 2 Validation Avg. Loss: 0.000034, Accuracy: 0.989800\n",
      "Epoch: 3 Train      Avg. Loss: 0.000289, Accuracy: 0.976633\n",
      "Epoch: 3 Validation Avg. Loss: 0.000055, Accuracy: 0.982000\n",
      "Epoch: 4 Train      Avg. Loss: 0.000265, Accuracy: 0.978500\n",
      "Epoch: 4 Validation Avg. Loss: 0.000038, Accuracy: 0.988300\n",
      "Epoch: 5 Train      Avg. Loss: 0.000260, Accuracy: 0.979950\n",
      "Epoch: 5 Validation Avg. Loss: 0.000030, Accuracy: 0.990700\n",
      "Epoch: 6 Train      Avg. Loss: 0.000241, Accuracy: 0.981117\n",
      "Epoch: 6 Validation Avg. Loss: 0.000025, Accuracy: 0.992800\n",
      "Epoch: 7 Train      Avg. Loss: 0.000231, Accuracy: 0.981450\n",
      "Epoch: 7 Validation Avg. Loss: 0.000097, Accuracy: 0.971200\n",
      "Epoch: 8 Train      Avg. Loss: 0.000225, Accuracy: 0.981783\n",
      "Epoch: 8 Validation Avg. Loss: 0.000028, Accuracy: 0.990600\n",
      "Epoch: 9 Train      Avg. Loss: 0.000214, Accuracy: 0.982933\n",
      "Epoch: 9 Validation Avg. Loss: 0.000039, Accuracy: 0.988100\n",
      "Epoch: 10 Train      Avg. Loss: 0.000226, Accuracy: 0.982483\n",
      "Epoch: 10 Validation Avg. Loss: 0.000024, Accuracy: 0.992900\n",
      "Epoch: 11 Train      Avg. Loss: 0.000209, Accuracy: 0.983550\n",
      "Epoch: 11 Validation Avg. Loss: 0.000025, Accuracy: 0.993200\n",
      "Epoch: 12 Train      Avg. Loss: 0.000198, Accuracy: 0.984567\n",
      "Epoch: 12 Validation Avg. Loss: 0.000027, Accuracy: 0.992100\n",
      "Epoch: 13 Train      Avg. Loss: 0.000193, Accuracy: 0.985117\n",
      "Epoch: 13 Validation Avg. Loss: 0.000022, Accuracy: 0.993200\n",
      "Epoch: 14 Train      Avg. Loss: 0.000201, Accuracy: 0.984500\n",
      "Epoch: 14 Validation Avg. Loss: 0.000030, Accuracy: 0.990300\n",
      "Epoch: 15 Train      Avg. Loss: 0.000187, Accuracy: 0.985400\n",
      "Epoch: 15 Validation Avg. Loss: 0.000021, Accuracy: 0.993700\n"
     ]
    }
   ],
   "source": [
    "# tips：利用各种方式提升模型准确率，如修改超参、调整网络结构、或做数据增强\n",
    "# tips: 优化的目标：1、从效果上优化 2、从速度上优化\n",
    "#\n",
    "# code：\n",
    "import visdom\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "if not os.path.exists('./model/'):\n",
    "    os.makedirs('./model/')\n",
    "viz = visdom.Visdom()\n",
    "\n",
    "best_acc = 0\n",
    "data_train_1 = MNIST('./data/mnist',\n",
    "                     download=True,\n",
    "                     transform=transforms.Compose([\n",
    "                         transforms.Resize((32, 32)),\n",
    "                         transforms.ToTensor(),\n",
    "                         #TODO\n",
    "                         transforms.RandomAffine(degrees=10,translate=(0.1,0.1),scale=(0.8,1.2),shear=10,fillcolor=0)]))\n",
    "\n",
    "data_val = MNIST('./data/mnist',\n",
    "                 train=False,\n",
    "                 download=True,\n",
    "                 transform=transforms.Compose([\n",
    "                      transforms.Resize((32, 32)),\n",
    "                      transforms.ToTensor()]))\n",
    "#TODO\n",
    "# print(data_train_1[1])\n",
    "data_train_loader = DataLoader(data_train_1,batch_size=256,shuffle=True,num_workers=8)#此处可以考虑数据增强，比如合并，旋转，平移，构造一些新的数据。不做数据增强的话就复制上一题的代码在这里。\n",
    "\n",
    "# for i in range(10):\n",
    "#     plt.imshow(data_train_1[i][0].view(32,32))\n",
    "#     plt.show()\n",
    "data_val_loader = DataLoader(data_val, batch_size=1024, num_workers=8)\n",
    "# print(data_val_loader.size())\n",
    "\n",
    "\n",
    "\n",
    "class optimalLeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(optimalLeNet5, self).__init__()\n",
    "        #网络结构的优化，比如加入BN层？\n",
    "        #TODO\n",
    "        self.convnet = nn.Sequential(OrderedDict([\n",
    "            ('bn0',nn.BatchNorm2d(1)),\n",
    "            ('c1', nn.Conv2d(1, 6, kernel_size=(5, 5))),\n",
    "            ('bn1',nn.BatchNorm2d(6)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('s2', nn.MaxPool2d(kernel_size=(2, 2), stride=2)),\n",
    "            ('c3', nn.Conv2d(6, 16, kernel_size=(5, 5))),\n",
    "            ('bn3',nn.BatchNorm2d(16)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('s4', nn.MaxPool2d(kernel_size=(2, 2), stride=2)),\n",
    "            ('c5', nn.Conv2d(16, 120, kernel_size=(5, 5))),\n",
    "            ('bn5',nn.BatchNorm2d(120)),\n",
    "            ('relu5', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "        self.fc = nn.Sequential(OrderedDict([\n",
    "            # ('bn5',nn.BatchNorm1d(120)),\n",
    "            ('f6', nn.Linear(120, 84)),\n",
    "            # ('bn6',nn.BatchNorm1d(84)),\n",
    "            ('relu6', nn.ReLU()),\n",
    "            ('f7', nn.Linear(84, 10)),\n",
    "            ('sig7', nn.LogSoftmax(dim=-1))\n",
    "        ]))\n",
    "    def forward(self, img):\n",
    "        #TODO\n",
    "        output = self.convnet(img)\n",
    "        output = output.view(img.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "def init_parameters(n):\n",
    "    if isinstance(n, nn.Linear):\n",
    "        nn.init.xavier_uniform_(n.weight)\n",
    "\n",
    "        \n",
    "        \n",
    "net = optimalLeNet5().cuda()\n",
    "net.apply(init_parameters)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#TODO\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)#尝试修改learningRate？换个optimizer？\n",
    "# optimizer=optim.SGD(net.parameters(),lr=0.01)\n",
    "\n",
    "# for visualization\n",
    "train_win = None\n",
    "win_opts = {\n",
    "    'title': 'Epoch Loss Trace',\n",
    "    'xlabel': 'Epoch Number',\n",
    "    'ylabel': 'Loss',\n",
    "    'width': 1200,\n",
    "    'height': 600,\n",
    "}\n",
    "def train(epoch):\n",
    "    global train_win\n",
    "    net.train()\n",
    "    loss_list, batch_list = [], []\n",
    "    avg_loss=0.0#总loss\n",
    "    total_correct=0\n",
    "    for i, (images, labels) in enumerate(data_train_loader):\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(images)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        avg_loss+=loss.detach().cpu().sum().item()\n",
    "        pred = output.detach().max(1)[1]\n",
    "        total_correct += pred.eq(labels.view_as(pred)).detach().sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss/=len(data_train_1)#平均loss\n",
    "    print('Epoch: %d Train      Avg. Loss: %f, Accuracy: %f'%(epoch,avg_loss,total_correct/len(data_train_1)))\n",
    "    if viz.check_connection():\n",
    "        train_win=viz.line(Y=torch.Tensor([avg_loss]),X=torch.Tensor([epoch]),win=train_win,\n",
    "                                 name='train_loss',update=None if train_win is None else 'append',\n",
    "                                 opts=win_opts)#将‘replace’修改为‘append’\n",
    "def validate(num):\n",
    "    global best_acc,train_win\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    avg_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(data_val_loader):\n",
    "        images=images.cuda()\n",
    "        labels=labels.cuda()\n",
    "        output = net(images)\n",
    "        avg_loss += criterion(output, labels).sum().item()\n",
    "        pred = output.detach().max(1)[1]\n",
    "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
    "        # if num==EPOCHS-1:\n",
    "        #     for j,lb in enumerate(labels):\n",
    "        #         if labels[j]!=pred[j]:\n",
    "        #             plt.imshow(images[j].cpu().view(32,32))\n",
    "        #             plt.show()\n",
    "        #             print(f'ans:{labels[j].item()},pred:{pred[j].item()}')\n",
    "    avg_loss /= len(data_val)\n",
    "    print('Epoch: %d Validation Avg. Loss: %f, Accuracy: %f' % (num,avg_loss, float(total_correct) / len(data_val)))\n",
    "    if viz.check_connection():\n",
    "        train_win=viz.line(Y=torch.Tensor([avg_loss]),X=torch.Tensor([num]),win=train_win,\n",
    "                                 name='test_loss',update=None if train_win is None else 'append',\n",
    "                                 opts=win_opts)#将‘replace’修改为‘append’\n",
    "    if float(total_correct) / len(data_val) > best_acc:\n",
    "        best_acc = float(total_correct) / len(data_val)\n",
    "        torch.save(net.state_dict(), './model/best_model.pt')\n",
    "EPOCHS=16\n",
    "def main():\n",
    "    #TODO\n",
    "    for epoch in range(EPOCHS):\n",
    "        global data_train_1,data_train_loader\n",
    "        data_train_1 = MNIST('./data/mnist',\n",
    "                     download=True,\n",
    "                     transform=transforms.Compose([\n",
    "                         transforms.Resize((32, 32)),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.RandomAffine(degrees=10,translate=(0.1,0.1),scale=(0.8,1.2),shear=10,fillcolor=0)]))\n",
    "        data_train_loader = DataLoader(data_train_1,batch_size=256,shuffle=True,num_workers=8)#此处可以考虑数据增强，比如合并，旋转，平移，构造一些新的数\n",
    "        train(epoch)\n",
    "        validate(epoch)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、visdom练习\n",
    "\n",
    "填补以下的visdom模块并运行得到结果。\n",
    "参考：https://github.com/facebookresearch/visdom\n",
    "\n",
    "**和上一题放在同一个env里保存，把env(json文件)和ipynb文件一起提交。（内容不限，画想画的就可以）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 158, 133)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'window_3920935c590602'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "# 图片\n",
    "# 单张图片\n",
    "import cv2\n",
    "img=cv2.imread('cat.jpg',cv2.IMREAD_COLOR)\n",
    "img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "img=np.transpose(img,(2,0,1))\n",
    "print(img.shape)\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "#your code here\n",
    "viz.image(\n",
    "    #your code here\n",
    "    img,\n",
    "    opts=dict(title='不学啦！',caption='不学啦！',width=500,height=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 散点图\n",
    "\n",
    "X=np.random.rand(100,3)\n",
    "Y=np.random.randint(1,3,100)\n",
    "#your code here\n",
    "old_scatter = viz.scatter(\n",
    "#your code here\n",
    "    X=X,Y=Y,\n",
    "    opts=dict(legend=['pwp','qwq'],markersize=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'window_3920935c60b0f0'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "\n",
    "# 柱状图\n",
    "\n",
    "\n",
    "#your code here\n",
    "viz.bar(\n",
    "#your code here\n",
    "    X=np.random.rand(10)\n",
    ")\n",
    "\n",
    "\n",
    "#your code here\n",
    "viz.bar(\n",
    "#your code here\n",
    "    X=np.random.rand(10,3),\n",
    "    opts=dict(\n",
    "        stacked=True,\n",
    "        legend=['pwp','qwq','pwq'],\n",
    "        rownames=list(range(2011,2021))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'window_3920935c64d372'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# 热力图\n",
    "\n",
    "#your code here\n",
    "viz.heatmap(\n",
    "#your code here\n",
    "    X=np.outer(np.arange(1,50),np.arange(1,50))\n",
    "    # X=data_train_1[0][0].view(32,32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'window_3920935c68b64a'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# 地表图\n",
    "x=np.random.rand(50)\n",
    "y=np.random.rand(50)\n",
    "for i in range(1,50):\n",
    "    x[i]+=x[i-1]\n",
    "    y[i]+=y[i-1]\n",
    "#your code here\n",
    "viz.contour(\n",
    "#your code here\n",
    "    X=np.outer(x,y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'window_3920936e7bf35e'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 表面图\n",
    "x=cv2.getGaussianKernel(51,6)\n",
    "y=cv2.getGaussianKernel(51,9)\n",
    "x=x@x.T\n",
    "y=y@y.T\n",
    "#your code here\n",
    "viz.surf(\n",
    "    #your code here\n",
    "    X=x-y,\n",
    "    opts=dict(title='Difference of Gaussian')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、（选做）可视化：Class Activation Map(CAM)\n",
    "运行CAM或grad-CAM的代码并展示效果。数据图片不限，但记得要使用分类任务的数据。\n",
    "\n",
    "不要求完全自己实现，推荐使用开源实现（也可以不用pytorch）。请用注释标注计算热力图权重的关键模块，并写出自己对关键模块的理解。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}