//utf-8
# 第二次作业报告 胡晋侨
## 一.之前的尝试:
+ 在最开始的程序(a.py)中,根据中文论文中的t测试差的概念,设置一阈值,将所有t测试差小于该阈值的间隔断开从而实现分词,并在此基础上统计词频,从而得到词典  
+ 又尝试了一种自己想出来的算法:利用相邻字的点互信息与每个字左右的字的熵加权求和,设置一阈值,断开和小于阈值的间隔,从而分词,统计词频  
+ 但实际测试中发现这两种算法在没有外置词典的情况下分词效果较差,于是转向了英文论文,并将其中的方法予以实践

## 二.算法流程(基本与论文中的相同)
1. 先读入所有语料并预处理出所有备选的关键词,并统计它们的几个参数(PMI,PKL,IDF等等)  
2. 读入标记的关键词库,并利用随机森林学习,随后用随机森林对所有关键词进行质量估计  
3. 利用质量估计值进行DP分词,并迭代直至$\theta$收敛,同时获得每个词的两个额外参数,将这两个参数追加至参数列表中  
4. 重复过程2,3  
5. 将所有词按照分词后的词频排序,并挑选出排名靠前的词,即为该语料的关键词

## 三.代码分布:
### 主要部分:
1. 读入并进行初始化的部分init.py:去除语料中所有回车符,将所有非汉字字符转换为空格,并初步找出所有出现次数>=3的短语,计算他们的PMI,PKL,IDF,边缘熵等参数(计算IDF时将一则报道视为一个文档),输出到2个文件中:copora.out保存转换完的语料,dict.out保存频繁出现的短语以及它们的频数等参数  
2. 估计所有短语的$Quality$值的部分getq.py:从train.out中读入手动生成的400个短语样本(合法/不合法各200个),并利用随机森林进行学习,再以此为基础,对dict.out中的短语的$Quality$值进行估计,将估计结果输出到getq.out中  
3. 进行DP分词,统计词频,迭代等操作的核心部分solve.cpp:从copora_cpp.out中读入语料和词表,不断进行DP和统计词频的迭代操作,并二分最优alpha,输出到3个文件中:cuts_cpp.out中保存分完词的语料,words_cpp.out中按词频降序保存所有词,features_cpp.out中保存根据分词结果得到每个词新的参数,用于反馈学习

### 辅助部分:
1. gen.py:从dict.out中读入所有短语,并随机抽词输出到屏幕上,人工判断其是否为合法的短语,将判断结果输出到train.out中  
2. pytocpp.py:将汉字转换为数字,将copora.out,dict.out,getq.out中的内容输出到copora_cpp.out中,便于solve.cpp处理,并将转换结果保存在label.out中  
3. cpptopy.py:将cuts_cpp.out,words_cpp.out,features_cpp.out中的内容"翻译"为中文,并分别输出到cuts.out,words.out,dict.out中,为最终结果  
4. compare.py:从words_xin.out,words_zbn.out中读入.于比较两份语料的用词异同,将结果输出到words_xin_only.out,words_zbn_only.out,words_both.out中  
5. main.py:负责控制所有程序的运行顺序

如此分配是因为python能方便地处理中文,调用sklearn库,而c++占用内存较少,运行时间较快,而电脑的资源有限,故这种方法权衡了编程效率与时间空间效率.(最开始使用solve.py来进行dp时,预计程序需要运行6个小时..故后来又编写了solve.cpp来进行dp,大概总共只需要运行40分钟)

## 四.参数设置:
1. 在原论文的基础上,除了PMI,IDF等之外,对每个短语增加了一个参数$H$,表示它两侧相邻的汉字不确定度的较小值(使用熵的公式计算),该值越小,则短语越不可能为合法  
2. 经过多轮实验,将$r_0$设为0.99  
3. 将原论文中对$\alpha$直接二分的过程改为对其对数进行二分的过程,范围为-10~10,对应$\alpha$从$2^{-10}$到$2^{10}$,二分17次左右就能收敛  
4. 由于要使用随机森林分类器,加载了sklearn模块,为了提高部分python代码的执行效率,使用pypy3代替默认的python3解释器运行  
5. 在分词的过程中,强制将'的'左右断开  
6. 为了尽可能发掘关键词,将统计高频词时的词频阈值设为3

## 五.测试环境:
+ 本地测试环境为ubuntu18.04lts,64位,CPU为i7-8550U,内存为7.7GB  
+ 完整运行一次大约需要40分钟(不含人工短语分类的时间)  
+ 运行环境需要支持python3,pypy3,g++,以及python3的sklearn库  
+ 使用pypy3 main.py或python3 main.py来执行代码  
+ 从xin_cmn_\*或zbn_cmn_\*中读取语料,并将词典及词频输出到words.out中  
可通过修改init.py第110行的内容来选择读取的语料

## 六.输出结果:
+ 先从两份语料的词表(words_xin.out,words_zbn.out)中按词频各取前1000名形成两个列表,然后将这些词语分为3类:在两个列表中都出现的,仅在新华社中出现的和仅在南华早报中出现的,分别输出到words_both.out,words_xin_only.out,words_zbn_only.out中

### 1.在两份语料中都有出现的高频词(前100名):
国际,中国,进行,美国,发展,目前,工作,今年,他们,企业,香港,问题,政府,我们,上海,一个,同时,通过,表示,伊拉克,经济,其中,要求,以及,北京,开始,举行,合作,计划,项目,由于,一些,这些,活动,比赛,建设,加强,自己,参加,亿美元,市场,公司,投资,日本,组织,没有,国家,希望,今天,决定,记者,提高,方面,研究,影响,根据,包括,增加,提供,发现,建立,认为,已经,双方,管理,获得,去年,分别,有关,提出,实施,需要,世界,宣布,城市,继续,支持,服务,技术,因此,如果,万元,社会,俄罗斯,代表,生产,出现,会议,全国,造成,努力,现在,强调,必须,规定,情况,报道,地区,安全,联合国

+ 可见两份报纸都对国际形势,经济建设等问题较为关注

### 2.仅在新华社中出现的高频词(前100名):
日电,新华社,非典,新华社北京,亿元,可吸入颗粒物,体育,我国,开展,防治,促进,澳门,港澳台,重点,中国队,人们,农民,多云,万人,开发,之一,电视快讯,北京市,实现,巴格达,实行,天津,加快,据介绍,共同,小标题,深圳,此次,日晚,农业,基地,最大,运动员,按照,今日新闻,农村,各地,群众,全省,人死亡,选手,召开,全面,日上午,新华社华盛顿,先后,确定,奥运会,再次,重庆,扩大,三个代表,西藏,具有,科学,保障,越来越,领导人,下降,制定,万吨,巴西,机制,基础上,新疆,莫斯科,同比增长,日下午,广西,签署,相关,非洲,西部,经济发展,截至,有关部门,产业,南京,新世纪,人受伤,培训,国务院,路线图,统一,代表团,成都,面临,规划,启动,外事,全市,贸易,创造,个国家,浙江

+ 可见新华社的报道部分侧重于国家大事以及经济,政治的话题,相对而言较为严肃

### 3.仅在南华早报中出现的高频词(前100名):
他说,但是,除了,当时,所以,昨天,这个,其实,她说,就是,本地,还是,然而,沙斯,另一方面,朋友,万新元,对于,成为,被告,之前,还有,所有,一名,当然,至于,例如,感觉,之外,什么,母亲,作为,死者,只是,面对,也是,他指出,为了,电影,角色,个月,父亲,所谓,曼联,即使,都是,丈夫,节目,公众,综合电,儿子,新航,妻子,新闻,父母,来说,老师,新元,女儿,免费,不是,过后,澳洲,故事,顾客,音乐,当局,员工,昨日,两人,酒店,肯定,地点,日前,后来,然后,东西,哥哥,联合早报,生意,等等,前天,这名,看法,一次,张国荣,女佣,课程,一切,做法,手机,马国,反应,事情,妈妈,突然,大约,昨晚,读者,两个

+ 可见南华早报的报道部分侧重于家庭生活以及娱乐,八卦的话题,相对而言较为活泼

### 4.语言习惯差异:
1. 新华社的'非典',在南华早报中则称为'沙斯'  
2. 南华早报的高频词中有大量的关联词以及口语化的词语,可能是因为使用讲故事的口吻进行叙述;而新华社的高频词中则很少有口语,更贴近书面语体