一.在最开始的程序(a.py)中,根据中文论文中的t测试差的概念,将所有汉字的间隔按照t测试差排序,并设置一阈值,将所有t测试差小于该阈值的间隔断开从而实现分词,并在此基础上统计词频,从而得到词典
但实际测试中发现这一算法在没有外置词典的情况下分词效果较差,于是转向了英文论文,并将其中的方法予以实践

二.源代码主要分为3部分,并有几个辅助部分:
1.读入并进行初始化的部分init.py:首先去除语料中所有回车符,再将所有非汉字字符转换为空格,并初步找出所有出现次数>=3的短语,并计算他们的PMI,KL散度,IDF,边缘熵等参数,输出到2个文件中:copora.out保存转换完的语料,dict.out保存频繁出现的短语以及它们的频数等参数
2.估计所有短语的quality值的部分getq.py:从train.out中读入手动生成的400个短语样本(合法/不合法各200个),并利用随机森林进行学习,再以此为基础,对dict.out中的短语的quality值进行估计,将估计结果输出到getq.out中
3.进行DP分词,统计词频,迭代等操作的核心部分solve.cpp:从copora_cpp.out中读入语料和词表,不断进行DP和统计词频的迭代操作,并二分最优alpha,输出到3个文件中:cuts_cpp.out中保存分完词的语料,words_cpp.out中按词频降序保存所有词,features_cpp.out中保存根据分词结果得到每个词新的参数,用于反馈学习

辅助部分:
1.gen.py:从dict.out中读入所有短语,并随机抽词输出到屏幕上,人工判断其是否为合法的短语,将判断结果输出到train.out中
2.pytocpp.py:将汉字转换为数字,将copora.out,dict.out,getq.out中的内容输出到copora_cpp.out中,便于solve.cpp处理,并将转换结果保存在label.out中
3.cpptopy.py:将cuts_cpp.out,words_cpp.out,features_cpp.out中的内容"翻译"为中文,并分别输出到cuts.out,words.out,dict.out中,为最终结果
5.main.py:负责控制所有程序的运行顺序

如此分配是因为python能方便地处理中文,而c++占用内存较少,运行时间较快,而电脑的资源有限,故这种方法权衡了编程效率与时间空间效率.(最开始使用solve.py来进行dp时,程序运行了足足3个小时..故后来又编写了solve.cpp来进行dp,大概总共只需要运行20分钟)

三.参数设置:
1.在原论文的基础上,除了pmi,idf等之外,对每个短语增加了一个参数h,表示它两侧相邻的汉字不确定度的较小值(使用熵的公式计算),该值越小,则短语越不可能为合法
2.由于新闻类语料的随机性较大,将r0设为0.97
3.将原论文中对alpha直接二分的过程改为对其对数进行二分的过程,范围为-10~10,对应alpha从2^-10到2^10,二分17次左右就能收敛
4.由于要使用随机森林分类器,加载了sklearn模块,为了提高部分python代码的执行效率,使用pypy3代替默认的python3解释器运行
5.在分词的过程中,强制将'的'左右断开
6.为了尽可能发掘关键词,将统计高频词时的词频阈值设为3

四.测试环境:
本地测试环境为ubuntu18.04lts,64位,CPU为i7-8550U,内存为7.7GB
完整运行一次大约需要40分钟(不含人工短语分类的时间)
运行环境需要支持python3,pypy3,g++,以及python3的sklearn库
使用pypy3 main.py或python3 main.py来执行代码
从xin_cmn_*或zbn_cmn_*中读取语料,并将词典及词频输出到words.out中
可通过修改init.py第110行的内容来选择读取的语料

五.输出结果:
1.新华社语料的关键词以及词频统计结果列在words_xin.out中,前100名中可以发现一些热点词汇,如:
国际,中国,美国,发展,新华社,目前,工作,非典,企业,香港,问题,政府,上海,伊拉克,经济,北京,举行,合作,亿元,计划,项目,活动,比赛,建设,加强,参加,亿美元,可吸入颗粒物,市场,公司,投资,日本,组织,国家,体育,希望,记者,提高,方面,研究,影响,双方,管理,实施,世界,城市,支持,服务,我国,技术,万元,社会,俄罗斯,代表,生产,会议,造成,努力,现在
2.